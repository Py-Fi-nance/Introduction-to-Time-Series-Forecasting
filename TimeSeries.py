# -*- coding: utf-8 -*-
"""2TimeSeries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fouEepORBOZJAV_jQz1cPahFOYWrMy4f
"""

!pip install openbb

from openbb_terminal.sdk import openbb

df_daily = openbb.stocks.load(symbol = 'ndaq')

df_daily.head()

df_daily.tail()

import matplotlib.pyplot as plt

# Assuming df_daily contains the daily stock data
df_daily['Close'].plot(figsize=(15,6), title='NASDAQ Daily Closing Prices')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.show()

# Define the original columns
original_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Dividends', 'Stock Splits']

# Select only the original columns from the DataFrame
df_daily = df_daily[original_columns]

missing_values = df_daily.isnull().sum()
print('Missing values:', missing_values)

df_daily.index = pd.to_datetime(df_daily.index)

# Extract the components
observed = seasonal_result.observed
trend = seasonal_result.trend
seasonal = seasonal_result.seasonal
residual = seasonal_result.resid

# Create subplots
fig, axes = plt.subplots(4, 1, figsize=(15, 12))
fig.subplots_adjust(hspace=0.4)

# Plot each component
axes[0].plot(observed)
axes[0].set_ylabel('Observed')
axes[0].set_title('Observed Component')
axes[1].plot(trend)
axes[1].set_ylabel('Trend')
axes[1].set_title('Trend Component')
axes[2].plot(seasonal)
axes[2].set_ylabel('Seasonal')
axes[2].set_title('Seasonal Component')
axes[3].plot(residual)
axes[3].set_ylabel('Residual')
axes[3].set_title('Residual Component')

plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
import pandas as pd

# Hyperparameter tuning for SMA
windows = [5, 10, 15, 20, 25, 30]

mse_sma_list, mae_sma_list = [], []

for window in windows:
    df_daily[f'SMA_{window}'] = df_daily['Close'].rolling(window=window).mean()

    mse_sma = mean_squared_error(df_daily['Close'][window-1:], df_daily[f'SMA_{window}'][window-1:])
    mae_sma = mean_absolute_error(df_daily['Close'][window-1:], df_daily[f'SMA_{window}'][window-1:])

    mse_sma_list.append(mse_sma)
    mae_sma_list.append(mae_sma)

print("SMA Hyperparameter Tuning Results:")
for win, mse, mae in zip(windows, mse_sma_list, mae_sma_list):
    print(f"Window Size: {win} -> MSE: {mse}, MAE: {mae}")

import matplotlib.pyplot as plt

# Assuming df_daily contains the daily stock data
best_window_size = 5
df_daily['SMA_5'] = df_daily['Close'].rolling(window=best_window_size).mean()

plt.figure(figsize=(15,6))
plt.plot(df_daily['Close'], label='Actual Close Price', color='yellow')
plt.plot(df_daily['SMA_5'], label='5-day SMA', color='red')
plt.title('NASDAQ Daily Closing Prices with 5-day SMA MSE: 0.83, MAE: 0.68')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.show()

# Function to calculate Exponential Weighted Moving Average
def ewma(weights):
    def calc(x):
        return (weights / weights.sum() * x).sum()
    return calc

mse_wma_list, mae_wma_list = [], []

# Assign exponential weights and calculate WMA for different window sizes
for window in windows:
    weights = np.exp(np.linspace(-1, 0, window))
    df_daily[f'EWMA_{window}'] = df_daily['Close'].rolling(window=window).apply(ewma(weights))

    mse_wma = mean_squared_error(df_daily['Close'][window-1:], df_daily[f'EWMA_{window}'][window-1:])
    mae_wma = mean_absolute_error(df_daily['Close'][window-1:], df_daily[f'EWMA_{window}'][window-1:])

    mse_wma_list.append(mse_wma)
    mae_wma_list.append(mae_wma)

print("WMA (Exponential Weights) Hyperparameter Tuning Results:")
for win, mse, mae in zip(windows, mse_wma_list, mae_wma_list):
    print(f"Window Size: {win} -> MSE: {mse}, MAE: {mae}")

plt.figure(figsize=(15,6))
plt.plot(df_daily['Close'], label='Actual Close Price', color='yellow')
plt.plot(df_daily['EWMA_5'], label='5-day WMA (Exponential Weights)', color='red')
plt.title('NASDAQ Daily Closing Prices with 5-day WMA | MSE: 0.53, MAE: 0.54')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.show()

import pandas as pd
from statsmodels.graphics.tsaplots import plot_acf
import matplotlib.pyplot as plt

# Assuming df_daily['Close'] is your time series data
series = df_daily['Close']

# Plot ACF at level
plt.figure(figsize=(12,6))
plot_acf(series, lags=40, title='Correlogram at Level')
plt.show()

# Compute 1st difference of the series
first_difference = series.diff().dropna()

# Plot ACF at 1st difference
plt.figure(figsize=(12,6))
plot_acf(first_difference, lags=40, title='Correlogram at 1st Difference')
plt.show()

import pandas as pd
from statsmodels.tsa.stattools import adfuller

def test_stationarity(timeseries):
    print('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])
    for key, value in dftest[4].items():
        dfoutput['Critical Value (%s)' % key] = value
    print(dfoutput)

test_stationarity(df_daily['Close'])

# Differencing the 'Close' column
df_daily['Close_diff'] = df_daily['Close'].diff()

# Dropping missing values created by differencing
df_daily = df_daily.dropna()

# You can plot the differenced series to visualize it
df_daily['Close_diff'].plot(figsize=(15,6), title='NASDAQ Daily Closing Prices (Differenced)')
plt.xlabel('Date')
plt.ylabel('Differenced Close Price')
plt.show()

from statsmodels.tsa.stattools import adfuller

result = adfuller(df_daily['Close_diff'])
print('ADF Statistic:', result[0])
print('p-value:', result[1])
print('Critical Values:')
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error

# Function to fit and evaluate ARIMA model
def evaluate_arima_model(df, arima_order):
    # Split into training and test sets (e.g., 80/20 split)
    split_idx = int(len(df) * 0.8)
    train, test = df[0:split_idx], df[split_idx:]
    history = [x for x in train]
    predictions = []

    # Step through the test set and make predictions
    for t in range(len(test)):
        model = ARIMA(history, order=arima_order)
        model_fit = model.fit()
        yhat = model_fit.forecast()[0]
        predictions.append(yhat)
        history.append(test[t])

    # Calculate MSE
    mse = mean_squared_error(test, predictions)
    return mse

# Grid search for ARIMA hyperparameters
p_values = range(0, 3)
d_values = range(0, 3)
q_values = range(0, 3)
best_mse = float('inf')
best_order = None

for p in p_values:
    for d in d_values:
        for q in q_values:
            order = (p, d, q)
            try:
                mse = evaluate_arima_model(df_daily['Close'], order)
                if mse < best_mse:
                    best_mse = mse
                    best_order = order
                print(f'ARIMA Order {order} - MSE: {mse}')
            except:
                continue

print(f'Best ARIMA Order: {best_order} - MSE: {best_mse}')

from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt

# Split into training and test sets (e.g., 80/20 split)
split_idx = int(len(df_daily['Close']) * 0.8)
train, test = df_daily['Close'][0:split_idx], df_daily['Close'][split_idx:]
history = [x for x in train]
predictions = []

# Step through the test set and make predictions using ARIMA(2, 1, 2)
for t in range(len(test)):
    model = ARIMA(history, order=(2, 1, 2))
    model_fit = model.fit()
    yhat = model_fit.forecast()[0]
    predictions.append(yhat)
    history.append(test.iloc[t])

# Create a DataFrame for predictions with the same index as the test set
predictions_df = pd.Series(predictions, index=test.index)

# Plot actual vs predicted
plt.figure(figsize=(12,6))
plt.plot(test, label='Actual')
plt.plot(predictions_df, label='Predicted', linestyle='dashed')
plt.title('ARIMA(2, 1, 2) Forecast vs Actual')
plt.xlabel('Time')
plt.ylabel('Close Price')
plt.legend()
plt.show()

from statsmodels.tsa.arima.model import ARIMA

model = ARIMA(df_daily['Close'], order=(2, 1, 2))
fit_model = model.fit()

print("AR Roots:", fit_model.arroots)
print("MA Roots:", fit_model.maroots)

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Assuming residuals is your data
fig = plt.figure(figsize=(12, 8))

# Plot ACF
ax1 = fig.add_subplot(211)
plot_acf(residuals, lags=range(1, 40), ax=ax1)
ax1.set_title('Autocorrelation Function')

# Plot PACF
ax2 = fig.add_subplot(212)
plot_pacf(residuals, lags=range(1, 40), ax=ax2)
ax2.set_title('Partial Autocorrelation Function')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Squaring the residuals
squared_residuals = residuals**2

# Creating the figure
fig = plt.figure(figsize=(12, 8))

# Plot ACF for squared residuals
ax1 = fig.add_subplot(211)
plot_acf(squared_residuals, lags=range(1, 40), ax=ax1)
ax1.set_title('Autocorrelation Function of Squared Residuals')

# Plot PACF for squared residuals
ax2 = fig.add_subplot(212)
plot_pacf(squared_residuals, lags=range(1, 40), ax=ax2)
ax2.set_title('Partial Autocorrelation Function of Squared Residuals')

plt.tight_layout()
plt.show()

from statsmodels.stats.diagnostic import acorr_ljungbox

lb_test = acorr_ljungbox(residuals, lags=[10])
print("Ljung-Box test:", lb_test)

from scipy.stats import kstest

ks_statistic, p_value = kstest(residuals, 'norm')
print("KS Statistic:", ks_statistic)
print("P-value:", p_value)

from statsmodels.stats.diagnostic import het_goldfeldquandt

gq_test = het_goldfeldquandt(residuals, fit_model.model.endog)
print("Goldfeld-Quandt test:", gq_test)